# -*- coding: utf-8 -*-
"""Victorization_simple_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BGtM3dIk8Xju5bEKcZjaHnkAwxyYgoFl

The implementation of sigmoid function
"""

import math



def basic_sigmoid(x):

    
    s = 1/(1+math.exp(-x))

    
    return s

print("basic_sigmoid(1) = " + str(basic_sigmoid(1)))

basic_sigmoid_test(basic_sigmoid)

"""Since we rarely use real number in deep learning, so we mostly use numpy instead of math """

### One reason why we use "numpy" instead of "math" in Deep Learning ###

x = [1, 2, 3] # x becomes a python list object
basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.
import numpy as np

# example of np.exp
t_x = np.array([1, 2, 3])
print(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))

t_x = np.array([1, 2, 3])
print (t_x + 3)

import numpy
def sigmoid(x):

    s = 1/(1+numpy.exp(-x))

    
    return s

t_x = np.array([1, 2, 3])
print("sigmoid(t_x) = " + str(sigmoid(t_x)))

sigmoid_test(sigmoid)

"""Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x.

𝑠𝑖𝑔𝑚𝑜𝑖𝑑_𝑑𝑒𝑟𝑖𝑣𝑎𝑡𝑖𝑣𝑒(𝑥)=𝜎′(𝑥)=𝜎(𝑥)(1−𝜎(𝑥))
"""

import numpy as np
def sigmoid_derivative(x):

    s = 1/(1+np.exp(-x))
    ds = s*(1-s)

    
    return ds
t_x = np.array([1, 2, 3])
print ("sigmoid_derivative(t_x) = " + str(sigmoid_derivative(t_x)))

"""Implement image2vector() that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). """

def image2vector(image):
    """
    Argument:
    image -- a numpy array of shape (length, height, depth)
    
    Returns:
    v -- a vector of shape (length*height*depth, 1)
    """
    
    # (≈ 1 line of code)
    # v =
    # YOUR CODE STARTS HERE
    v = image.reshape(image.shape[0] * image.shape[1]* image.shape[2],1)
    
    # YOUR CODE ENDS HERE
    
    return v

t_image = np.array([[[ 0.67826139,  0.29380381],
                     [ 0.90714982,  0.52835647],
                     [ 0.4215251 ,  0.45017551]],

                   [[ 0.92814219,  0.96677647],
                    [ 0.85304703,  0.52351845],
                    [ 0.19981397,  0.27417313]],

                   [[ 0.60659855,  0.00533165],
                    [ 0.10820313,  0.49978937],
                    [ 0.34144279,  0.94630077]]])

print ("image2vector(image) = " + str(image2vector(t_image)))

image2vector_test(image2vector)

import numpy as np
def normalize_rows(x):

    x_n=np.linalg.norm(x,axis=1, keepdims=True)
    print(x_n)
    x = x/x_n
 

    return x
x = np.array([[0, 3, 4],
              [1, 6, 4]])
print("normalizeRows(x) = " + str(normalize_rows(x)))

"""mplement a softmax function using numpy

𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑥)=𝑠𝑜𝑓𝑡𝑚𝑎𝑥([𝑥1𝑥2...𝑥𝑛])
"""

import numpy as np

def softmax(x):

    x_exp = np.exp(x)
    x_sum = np.sum(x_exp, axis = 1, keepdims = True)
    s = x_exp / x_sum
 
    
    return s
t_x = np.array([[9, 2, 5, 0, 0],
                [7, 5, 0, 0 ,0]])
print("softmax(x) = " + str(softmax(t_x)))

"""Implement the numpy vectorized version of the L1 loss. 

𝐿1(𝑦̂ ,𝑦)=∑𝑖=0𝑚−1|𝑦(𝑖)−𝑦̂ (𝑖)|
"""

def L1(yhat, y):

    # YOUR CODE STARTS HERE
    loss = np.sum(np.abs(y-yhat))

    
    return loss
yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L1 = " + str(L1(yhat, y)))

def L2(yhat, y):

    temp = (y-yhat)
    loss = np.sum(np.dot(temp,temp))
    

    
    return loss